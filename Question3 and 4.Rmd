---
title: "IDA Assignment 1"
author: "Aijia Huang s2036061"
output:
  word_document: default
  html_document: default
---

## 1.
### (a) (ii) 0.3
### The MCAR means the missing data distribution and observed data distribution and complete data distribution are similar. Further, MCAR implies that the probability of missing value for the variable is the same for all individuals. And 0.3 in question means into ALQ=YES, the probability of missing value is 0.3. Therefore, ALQ=No is the same distribution with ALQ=Yes, and the answer should be 0.3.

### (b) (ii) The probability of ALQ being missing is independent of the Yes/No value of ALQ after adjusting for gender.
### Since ALQ is MAR, hence the missing values in ALQ dosen't depend on themselves, and they should depend on other observed variable and data. So, (i) and (iii) are both incorrect.

###(c) (iii) It is impossible to conclude from the information given.
### The similar idea with question (a), however, MAR doesn't have the same distribution of missing data, observed data and complete data. Also, the probability of missing value for the variable is not the same for all individuals. Therefore, although we know the probability of ALQ being missing for men is 0.1, it is impossible to conclude the question from that.


## 2.The largest possible subsample is 90, and the smallest subsample is 0.
### The dataset consists of 100 subjects and 10 variables. Each variable contains 10% of missing values (10 missing values). If 10% missing values of each variable are the same position, there will be the largest possible subsample, 100*(1-10%) = 90. If 10% missing values of each variable are the different and completely unique position, there will be the smallest possible subsample, 100*(1-10*10%) = 0.


## 3.
### (a) MAR
### MAR:Since a=2 and b=0,hence in the missing condition: "a*(Y1-1)+b*(Y2-5)+Z3<0", there isn't any contributions of Y2. So the missing values of Y2 are unrelate to themselves and relate to Y1. Therefore, MAR.

### Comment: Under the MAR, as shown in the graph, the peak of the observed (after imposing missingness) data density moves right and become higher than the complete data, and the density of complete data and observed (after imposing missingness)  data are not similar. This is consistent with the rule of MAR mechanism, that complete data and observed data distribution are different.

```{r}
#Question 3
# (a)
set.seed(1)
n<- 500
mu1<-mu2<-mu3<-0
sigma1<-sigma2<-sigma3<-1
z1<-rnorm(n,mu1,sigma1)
z2<-rnorm(n,mu2,sigma2)
z3<-rnorm(n,mu3,sigma3)

Y1<-1+z1
Y2<-5+2*z1+z2

#MAR
Y2_MAR_obs <- Y2[2*z1 +z3 >= 0]
Y2_MAR_mis <- Y2[2*z1 +z3<0]

plot(density(Y2), lwd = 2, col = "blue", main = "MAR", ylim = c(0,0.3))
lines(density(Y2_MAR_obs), lwd = 2, col = "red")
legend(6, 0.3,legend = c("Complete data", "Observed data"),cex=0.8,
       col = c("blue", "red"), lty = c(1, 1), lwd = c(2, 2), bty = "n")

```

## (b)
### Comment: Imputing the missing values using stochastic regression imputation means here we use a regression model to predict the incomplete variables(Y2) from the complete variable(Y1), and then add noise to the predictions, which can restore lost variability to the data. Further, stochastic regression imputation can give unbiased parameter estimates under MAR. Hence, as shown in this graph, the density of complete data and data after imputation are similar compared with the graph in the Question(a). This also can show that the stochastic regression imputation approach is a great way to fill the missing data. And it can cause relatively less errors within this approach. 

```{r}
#Question 3
# (b)
set.seed(1)
n<- 500
mu1<-mu2<-mu3<-0
sigma1<-sigma2<-sigma3<-1
z1<-rnorm(n,mu1,sigma1)
z2<-rnorm(n,mu2,sigma2)
z3<-rnorm(n,mu3,sigma3)

Y1<-1+z1
Y2<-5+2*z1+z2

Y2c=Y2
Y2c[which((2*z1 +z3)<0)]=NA

data<-data.frame(Y1=Y1, Y2cc=Y2c)

fit <- lm(Y2cc ~ Y1, data = data) 

set.seed(1)
predicted_sri <- predict(fit, newdata = data) + rnorm(nrow(data), 0, sigma(fit))
Y2_sri <- ifelse(is.na(data$Y2cc) == TRUE, predicted_sri, data$Y2cc)

plot(density(Y2), lwd = 2, col = "blue", main = "Stochastic regression imputation", ylim = c(0,0.3))
lines(density(Y2_sri), lwd = 2, col = "red")
legend(6, 0.3,legend = c("Complete data", "Completed data(after imputation)"),cex=0.8,
       col = c("blue", "red"), lty = c(1, 1), lwd = c(2, 2), bty = "n")

```

### (c) MNAR
### MNAR:Since a=0 and b=2, hence in the missing condition: "a*(Y1-1)+b*(Y2-5)+Z3<0", there isn't any contributions of Y1. So the missing values of Y2 are unrelate to Y1 and relate to Y2. Therefore, MNAR.

### Comment: Under the MNAR, as shown in the graph, the peak of the observed (after imposing missingness) data density moves right and become higher than the complete data, and the density of complete data and observed (after imposing missingness) data are quite different. This is consistent with the rule of MNAR mechanism, that complete data and observed data distribution are different.
```{r}
#Question 3
# (c)
n<- 500
mu1<-mu2<-mu3<-0
sigma1<-sigma2<-sigma3<-1
z1<-rnorm(n,mu1,sigma1)
z2<-rnorm(n,mu2,sigma2)
z3<-rnorm(n,mu3,sigma3)

Y1<-1+z1
Y2<-5+2*z1+z2

#MNAR
Y2_MNAR_obs <- Y2[2*Y2-10+z3 >= 0]
Y2_MNAR_mis <- Y2[2*Y2-10+z3 < 0]

plot(density(Y2), lwd = 2, col = "blue", main = "MNAR", ylim = c(0,0.5))
lines(density(Y2_MNAR_obs), lwd = 2, col = "red")
legend(8, 0.5,legend = c("Complete data", "Observed data"),cex=0.8,
       col = c("blue", "red"), lty = c(1, 1), lwd = c(2, 2), bty = "n")
```

## Question 3
## (d)
### Comment: Under imputing the missing values using stochastic regression imputation, as shown in this graph, the density of complete data and data after imputation are relatively similar compared with graph in the Question(c). That implies the stochastic regression imputation is a great approach to fill the missing data. However, due to MNAR mechanism, they are still a bit different from the density of complete data and completed data after imputation. 

```{r}
#Question 3
# (d)
set.seed(1)
n<- 500
mu1<-mu2<-mu3<-0
sigma1<-sigma2<-sigma3<-1
z1<-rnorm(n,mu1,sigma1)
z2<-rnorm(n,mu2,sigma2)
z3<-rnorm(n,mu3,sigma3)

Y1<-1+z1
Y2<-5+2*z1+z2

Y2d=Y2
Y2d[(4*z1+2*z2+z3)<0]=NA

data_d<-data.frame(Y1=c(Y1), Y2dd=c(Y2d))

fit_d<- lm(Y2dd ~ Y1, data = data_d) 

set.seed(1)
predicted_sri_d <- predict(fit_d, newdata = data_d) + rnorm(nrow(data_d), 0, sigma(fit_d))
Y2_sri_d <- ifelse(is.na(data_d$Y2dd), predicted_sri_d, data_d$Y2dd)

plot(density(Y2), lwd = 2, col = "blue", main = "Stochastic regression imputation", ylim = c(0,0.27))
lines(density(Y2_sri_d), lwd = 2, col = "red")
legend(8, 0.27,legend = c("Complete data", "Completed data(after imputation)"),cex=0.8,
       col = c("blue", "red"), lty = c(1, 1), lwd = c(2, 2), bty = "n")

```

## 4.
### (a) 
###The mean value of the recovery time provided by a complete case analysis is 19.27273. 
###The associated standard error is 2.603013.
###The correlations between the recovery time and the dose is 0.2391256. 
###The correlations between the recovery time and the blood pressure is -0.01952862.

```{r}
load("databp.Rdata")
recov<- which(is.na(databp$recovtime) == FALSE)
merecov<-mean(databp$recovtime, na.rm = TRUE)
serecov<-sd(databp$recovtime, na.rm = TRUE)/sqrt(length(recov))

cor1<-cor(databp$recov, databp$logdose, use = "complete")
cor2<-cor(databp$recov, databp$bloodp, use = "complete")

merecov
serecov
cor1
cor2
```

### (b) 
###The mean value of the recovery time provided by a complete case analysis is 19.27273.
###The associated standard error is 2.284135.
###The correlations between the recovery time and the dose is 0.2150612.
###The correlations between the recovery time and the blood pressure is -0.01934126.

```{r}
re_mi <- ifelse(is.na(databp$recovtime), mean(databp$recovtime, na.rm = TRUE), databp$recovtime)

mearecov<-mean(re_mi)
searecov<-sd(re_mi)/sqrt(length(re_mi))
cor1_b<-cor(re_mi, databp$logdose)
cor2_b<-cor(re_mi, databp$bloodp)

mearecov
searecov
cor1_b
cor2_b
```

### (c) 
###The mean value of the recovery time provided by a complete case analysis is 19.44428.
###The associated standard error is 2.312845.
###The correlations between the recovery time and the dose is 0.2801835.
###The correlations between the recovery time and the blood pressure is -0.0111364.

```{r}
fit_c <- lm(recovtime ~ logdose+bloodp, data = databp) 
predicted_ri <- predict(fit_c, newdata = databp)
re_ri <- ifelse(is.na(databp$recovtime), predicted_ri, databp$recovtime)

merrecov<-mean(re_ri)
serrecov<-sd(re_ri)/sqrt(length(re_ri))
cor1_c<-cor(re_ri, databp$logdose)
cor2_c<-cor(re_ri, databp$bloodp)

merrecov
serrecov
cor1_c
cor2_c
```

### (d) 
###The mean value of the recovery time provided by a complete case analysis is 20.4598.
###The associated standard error is 2.444571.
###The correlations between the recovery time and the dose is 0.2284537.
###The correlations between the recovery time and the blood pressure is -0.01786944.

### As for stochastic regression imputation, with adding noise term, which can lead to relatively implausible predictions. For example, it makes no sense that impute a negative value for the Variables that are prone to missing. 

```{r}
fit_d <- lm(recovtime ~ logdose+bloodp, data = databp) 

set.seed(1)
predicted_sri <- predict(fit_d, newdata = databp) + rnorm(nrow(databp), 0, sigma(fit_d))
re_sri <- ifelse(is.na(databp$recovtime), predicted_sri, databp$recovtime)

mesrecov<-mean(re_sri)
sesrecov<-sd(re_sri)/sqrt(length(re_sri))
cor1_d<-cor(re_sri, databp$logdose)
cor2_d<-cor(re_sri, databp$bloodp)

mesrecov
sesrecov
cor1_d
cor2_d
```

### (e)
### As for this question, as shown in the output, first three lines output is the predicted values of each individuals in recovtime. Further, for the 4th, 10th and 22th missing value in recovtime should be imputed with the 6th, 2nd, and 17th value in the recovtime. And the imputed vales are 13, 10, 39 respectively. Finally, there is an output with the whole vales (after imputing) of recovtime.

```{r}
f<-function(logdose,bloodp){
  co<-coef(fit_c)
  print (co[[1]]+logdose*co[[2]]+bloodp*co[[3]])
}

pre_value<-f(databp[,1],databp[,2])
y<-databp$recovtime

for (i in 1:25){ 
  if (((pre_value[i]-pre_value[4])^2) == min((pre_value[-c(4,10,22)]-pre_value[4])^2)){
    print(i)
    y[4]<-y[i]
    print(y[4])
  }
}

for (i in 1:25){ 
  if (((pre_value[i]-pre_value[10])^2) == min((pre_value[-c(4,10,22)]-pre_value[10])^2)){
    print(i)
    y[10]<-y[i]
    print(y[10])
  }
}

for (i in 1:25){ 
  if (((pre_value[i]-pre_value[22])^2) == min((pre_value[-c(4,10,22)]-pre_value[22])^2)){
    print(i)
    y[22]<-y[i]
    print(y[22])
  }
}

print(y)
```


### (f)
#### Predictive mean matching over stochastic regression imputation will be more precision. And predictive mean matching can relatively fix the problem that stochastic regression attenuate statndard errors of the estimates as many single imputation.

### Firstly, there are many problems which are nonlinear, hence it will be more precision if this problem can be modified. Additionally, this approach maybe can produce substantially biased estimates of correlations and regression coefficients, which is another potential problem.  








